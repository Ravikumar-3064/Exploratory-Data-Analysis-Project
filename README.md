
                                                   EXPLORATORY DATA ANALYSIS

                                                                                 
Exploratory Data Analysis (EDA) is the initial step in data analysis where we explore datasets to understand their structure, patterns, and key characteristics.
It helps uncover relationships, anomalies, and trends before applying machine learning or statistical models.

âœ¨ Key Objectives of EDA:

ğŸ” Understand the dataset â€“ shape, size, data types, missing values.

ğŸ“ˆ Summarize data distributions â€“ mean, median, variance, outliers.

ğŸ§© Identify patterns & correlations between features.

ğŸš¨ Detect anomalies or unusual observations.

ğŸ¨ Visualize data for deeper insights using plots and graphs.

ğŸ› ï¸ Tools & Techniques:

Descriptive Statistics â†’ mean, median, mode, standard deviation.

Data Visualization â†’ histograms, scatter plots, boxplots, heatmaps.

Data Cleaning â†’ handling null values, duplicates, and outliers.

Feature Understanding â†’ identifying categorical vs numerical variables.


ğŸ§© PART 1 â€“ Web Scraping & Data Collection
ğŸ“š Libraries Used

ğŸ”— requests â†’ Fetches the websiteâ€™s HTML content.

ğŸ¥£ BeautifulSoup â†’ Parses the HTML to extract structured data.

ğŸ“‚ csv â†’ Saves the extracted data into a CSV file.

ğŸ—ï¸ Script Structure

ğŸ›¡ï¸ Uses a try...except block for error handling.

ğŸ“‹ Data stored in a list of dictionaries (author, quote, tags).

ğŸ”„ while loop iterates through pages to fetch quotes.

ğŸ” Scraping Process

Starts from page 1, continues until all pages are processed.

Constructs the page URL dynamically.

Uses requests.get() â†’ retrieves HTML.

BeautifulSoup extracts:

âœï¸ Author

ğŸ’¬ Quote text

ğŸ·ï¸ Tags

Each entry is stored as a dictionary â†’ appended to the list.

ğŸ“‘ Handling Multiple Pages

ğŸ”„ Script continues as long as thereâ€™s a "next" button.

ğŸ“Œ Current setup â†’ scrapes up to 10 pages.

âœ… Ensures all available quotes are collected.

ğŸ’¾ Saving to CSV

Exports results to quotes.csv.

Defines columns: author, quote, tag_name.

Uses csv.DictWriter to store rows.

ğŸ“Š Structured dataset ready for analysis.

ğŸ§© PART 2 â€“ SQL Queries on Quotes Data
ğŸ” Query 1 â€“ Count Quotes per Author
SELECT author, COUNT(*) AS quote_count
FROM quotes
GROUP BY author
ORDER BY quote_count DESC;


ğŸ‘‰ Shows authors ranked by number of quotes.

ğŸ·ï¸ Query 2 â€“ Top 5 Most Common Tags
SELECT tag_name, COUNT(tag_name) AS tag_count
FROM quotes
GROUP BY tag_name
ORDER BY tag_count DESC
LIMIT 5;


ğŸ‘‰ Retrieves top 5 tags with the highest frequency.

âœï¸ Query 3 â€“ Authors with More Than 5 Quotes
SELECT author, COUNT(author) AS quote_count
FROM quotes
GROUP BY author
HAVING COUNT(author) > 5;


ğŸ‘‰ Filters only authors with >5 quotes.

ğŸ“ Query 4 â€“ Find the Longest Quote
SELECT author, quote_text
FROM quotes
ORDER BY LENGTH(quote_text) DESC
LIMIT 1;


ğŸ‘‰ Returns the longest quote and its author.

ğŸ§© PART 3 â€“ Exploratory Data Analysis (EDA)
ğŸ“‚ Steps

ğŸ¼ import pandas as pd â†’ Import Pandas.

ğŸ“¥ pd.read_csv("quotes.csv") â†’ Load dataset.

ğŸ“ df.info() â†’ Summary (rows, cols, datatypes, null values).

ğŸ‘€ df.head() â†’ Preview first 5 rows.

ğŸ”¢ df['author'].nunique() â†’ Count of unique authors.

ğŸ“Š df.describe(include='all') â†’ Descriptive statistics for all columns.

âœ… Insights

ğŸ” Identifies missing values & datatypes.

âœ¨ Shows sample data rows.

ğŸ‘©â€ğŸ’» Finds number of distinct authors.

ğŸ“ˆ Provides statistical overview of dataset.
